# -*- coding: utf-8 -*-
"""DSML_Final Combined_18307.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wmF1Uf-4Ell6y9wueXuu4zNuaSEzuGQ2
"""

pip install AstroML

import os
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression
import warnings
warnings.filterwarnings('ignore')
from sklearn import svm
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import KMeans
from sklearn import datasets
from matplotlib import pyplot as plt
from tensorflow.keras.models import load_model
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import SGD
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.layers import LeakyReLU
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Dropout, BatchNormalization
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import (KNeighborsClassifier,NeighborhoodComponentsAnalysis)
from sklearn.pipeline import make_pipeline
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from astroML.datasets import get_data_home
from astroML.datasets.tools import sql_query
import matplotlib.pyplot as plt
from astroML.datasets import fetch_sdss_galaxy_colors
tf.random.set_seed(7)

#Using a Predefined Function

NOBJECTS = 10000

GAL_COLORS_NAMES = ['u', 'g', 'r', 'i', 'z', 'specClass',
                    'redshift', 'redshift_err']

ARCHIVE_FILE = 'sdss_galaxy_colors.npy'


def fetch_sdss_galaxy_colors(data_home=None, download_if_missing=True):
    """Loader for SDSS galaxy colors.
    This function directly queries the sdss SQL database at
    http://cas.sdss.org/
    Parameters
    ----------
    data_home : optional, default=None
        Specify another download and cache folder for the datasets. By default
        all astroML data is stored in '~/astroML_data'.
    download_if_missing : optional, default=True
        If False, raise a IOError if the data is not locally available
        instead of trying to download the data from the source site.
    Returns
    -------
    data : recarray, shape = (10000,)
        record array containing magnitudes and redshift for each galaxy
    """
    data_home = get_data_home(data_home)

    archive_file = os.path.join(data_home, ARCHIVE_FILE)

    query_text = ('\n'.join(("SELECT TOP %i" % 10000,
                             "  p.u, p.g, p.r, p.i, p.z, s.class, s.z, s.zerr",
                             "FROM PhotoObj AS p",
                             "  JOIN SpecObj AS s ON s.bestobjid = p.objid",
                             "WHERE ",
                             "  p.u BETWEEN 0 AND 20.6",
                             "  AND p.g BETWEEN 0 AND 21",
                             "  AND s.class <> 'UNKNOWN'",
                             "  AND s.class <> 'STAR'",
                             "  AND s.class <> 'SKY'",
                             "  AND s.class <> 'STAR_LATE'")))

    if not os.path.exists(archive_file):
        if not download_if_missing:
            raise IOError('data not present on disk. '
                          'set download_if_missing=True to download')

        print("querying for %i objects" % NOBJECTS)
        print(query_text)
        output = sql_query(query_text)
        print("finished.")

        kwargs = {'delimiter': ',', 'skip_header': 2,
                  'names': GAL_COLORS_NAMES, 'dtype': None,
                  'encoding': 'ascii',
                  }

        data = np.genfromtxt(output, **kwargs)
        np.save(archive_file, data)

    else:
        data = np.load(archive_file)

    return data

df1=fetch_sdss_galaxy_colors()
df1=pd.DataFrame(df1)
df1

df2=pd.read_excel(r"/content/18307_training.xlsx") #Import the training data set
df2.describe()

df_test=pd.read_excel(r"/content/18307_testing.xlsx")#Import the testing data set
df_test

X=df2.drop('class',axis=1)
X_s=X/X.apply(max)
X_s.describe()

y2=df2.iloc[:,5]# taken from labelled data
y_s=pd.DataFrame(y2)
scaled=[X_s,y_s]
df2_sc=pd.concat(scaled,axis=1)
sns.pairplot(data=df2_sc,hue='class')#Plotting Training Data

data = df2
data1 = data[::10]  # truncate for plotting

# Extract colors and spectral classG
ug = data['u'] - data['g']
gr = data['g'] - data['r']
spec_class = data['class']

galaxies = (spec_class == 'GALAXY')
qsos = (spec_class == 'QSO')
star = (spec_class == 'STAR')

#------------------------------------------------------------
# Prepare plot
fig = plt.figure()
ax = fig.add_subplot(111)

ax.set_xlim(-0.5, 2.5)
ax.set_ylim(-0.5, 1.5)

ax.plot(ug[galaxies], gr[galaxies], '.', ms=4, c='b', label='galaxies')
ax.plot(ug[qsos], gr[qsos], '.', ms=4, c='r', label='qsos')
ax.plot(ug[star],gr[star],'.',ms=4,c='y',label='star')

ax.legend(loc=2)

ax.set_xlabel('$u-g$')
ax.set_ylabel('$g-r$')

plt.show()#captures somewhat  linear relation

zi = data['z'] - data['i']
uz = data['u'] - data['z']
fig = plt.figure()
ax = fig.add_subplot(111)

ax.set_xlim(-0.5, 2.5)
ax.set_ylim(-0.5, 1.5)

ax.plot(zi[galaxies], uz[galaxies], '.', ms=4, c='b', label='galaxies')
ax.plot(zi[qsos], uz[qsos], '.', ms=4, c='r', label='qsos')
ax.plot(zi[star],uz[star],'.',ms=4,c='y',label='star')

ax.legend(loc=2)

ax.set_xlabel('$z-i$')
ax.set_ylabel('$u-z$')
plt.legend()
plt.show()

#3-D PLots
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
col=df2['class'].map({'STAR':0,'QSO':1,'GALAXY':2})
colors = ['orange','black','yellow']
fig = plt.figure(figsize=(8,8))
ax=fig.add_subplot(111,projection='3d')
plt.scatter(df2.u,df2.g,df2.r, c=col, cmap=matplotlib.colors.ListedColormap(colors))
ax.set_xlabel('$u$',fontsize="20")
ax.set_ylabel('$g$',fontsize="20")
ax.set_zlabel('$r$',fontsize="20")
cb = plt.colorbar()
cb.set_ticks([0,1,2])
cb.set_ticklabels(["STAR", "QSO", "GALAXY"])
plt.show()

fig = plt.figure(figsize=(9,9))
ax=fig.add_subplot(111,projection='3d')
plt.scatter(df2.r,df2.i,df2.redshift, c=col, cmap=matplotlib.colors.ListedColormap(colors))
ax.set_xlabel('$r$',fontsize="20")
ax.set_ylabel('$i$',fontsize="20")
ax.set_zlabel('$redshift$',fontsize="20")
cb = plt.colorbar()
cb.set_ticks([0,1,2])
cb.set_ticklabels(["STAR", "QSO", "GALAXY"])
plt.show()

fig = plt.figure(figsize=(8,8))
ax=fig.add_subplot(111,projection='3d')
plt.scatter(df2.r,df2.i,df2.z, c=col, cmap=matplotlib.colors.ListedColormap(colors))
ax.set_xlabel('$r$',fontsize="20")
ax.set_ylabel('$i$',fontsize="20")
ax.set_zlabel('$z$',fontsize="20")
cb = plt.colorbar()
cb.set_ticks([0,1,2])
cb.set_ticklabels(["STAR", "QSO", "GALAXY"])
plt.show()

fig = plt.figure(figsize=(8,8))
ax=fig.add_subplot(111,projection='3d')
plt.scatter(df2.u,df2.r,df2.z, c=col, cmap=matplotlib.colors.ListedColormap(colors))
ax.set_xlabel('$u$',fontsize="20")
ax.set_ylabel('$r$',fontsize="20")
ax.set_zlabel('$z$',fontsize="20")
cb = plt.colorbar()
cb.set_ticks([0,1,2])
cb.set_ticklabels(["STAR", "QSO", "GALAXY"])
plt.show()

fig = plt.figure(figsize=(8,8))
ax=fig.add_subplot(111,projection='3d')
plt.scatter(df2.g,df2.i,df2.redshift, c=col, cmap=matplotlib.colors.ListedColormap(colors))
ax.set_xlabel('$g$',fontsize="20")
ax.set_ylabel('$i$',fontsize="20")
ax.set_zlabel('$redshift$',fontsize="20")
cb = plt.colorbar()
cb.set_ticks([0,1,2])
cb.set_ticklabels(["STAR", "QSO", "GALAXY"])
plt.show()

ytann=df_test['class']
ytann

X_t=df_test.drop('class',axis=1)
X_t_sc=X_t/X_t.apply(max)

y_t=df_test.iloc[:,5]
y_t_sc=pd.DataFrame(y_t)
scaled=[X_t_sc,y_t_sc]
dftest_sc=pd.concat(scaled,axis=1)
sns.pairplot(data=dftest_sc,hue='class')#Plotting Testing Data

data = df_test
data1 = data[::10]  # truncate for plotting

# Extract colors and spectral classG
ug = data['u'] - data['g']
gr = data['g'] - data['r']
spec_class = data['class']

galaxies = (spec_class == 'GALAXY')
qsos = (spec_class == 'QSO')
star = (spec_class == 'STAR')

#------------------------------------------------------------
# Prepare plot
fig = plt.figure()
ax = fig.add_subplot(111)

ax.set_xlim(-0.5, 2.5)
ax.set_ylim(-0.5, 1.5)

ax.plot(ug[galaxies], gr[galaxies], '.', ms=4, c='b', label='galaxies')
ax.plot(ug[qsos], gr[qsos], '.', ms=4, c='r', label='qsos')
ax.plot(ug[star],gr[star],'.',ms=4,c='y',label='star')

ax.legend(loc=2)

ax.set_xlabel('$u-g$')
ax.set_ylabel('$g-r$')

plt.show()

zi = data['z'] - data['i']
uz = data['u'] - data['z']
fig = plt.figure()
ax = fig.add_subplot(111)

ax.set_xlim(-0.5, 2.5)
ax.set_ylim(-0.5, 1.5)

ax.plot(zi[galaxies], uz[galaxies], '.', ms=4, c='b', label='galaxies')
ax.plot(zi[qsos], uz[qsos], '.', ms=4, c='r', label='qsos')
ax.plot(zi[star],uz[star],'.',ms=4,c='y',label='star')

ax.legend(loc=2)

ax.set_xlabel('$z-i$')
ax.set_ylabel('$u-z$')
plt.legend()
plt.show()

fig = plt.figure(figsize=(8,8))
ax=fig.add_subplot(111,projection='3d')
plt.scatter(df_test.r,df_test.i,df_test.z, c=col, cmap=matplotlib.colors.ListedColormap(colors))
ax.set_xlabel('$r$',fontsize="20")
ax.set_ylabel('$i$',fontsize="20")
ax.set_zlabel('$z$',fontsize="20")
cb = plt.colorbar()
cb.set_ticks([0,1,2])
cb.set_ticklabels(["STAR", "QSO", "GALAXY"])
plt.show()

fig = plt.figure(figsize=(8,8))
ax=fig.add_subplot(111,projection='3d')
plt.scatter(df_test.u,df_test.r,df_test.z, c=col, cmap=matplotlib.colors.ListedColormap(colors))
ax.set_xlabel('$u$',fontsize="20")
ax.set_ylabel('$r$',fontsize="20")
ax.set_zlabel('$z$',fontsize="20")
cb = plt.colorbar()
cb.set_ticks([0,1,2])
cb.set_ticklabels(["STAR", "QSO", "GALAXY"])
plt.show()

sns.set_style('whitegrid');#To emphasize linear relationship
sns.FacetGrid(df2_sc, hue='class', height=5) \
.map(plt.scatter, 'u', 'g') \
.add_legend();
plt.show()







scalar=StandardScaler().fit(X)
X_scaled=scalar.transform(X)
X_scaled = pd.DataFrame(X_scaled)
X_scaled

#Mapping Classes to Integers
df2_sc['class'] = df2_sc['class'].map({'STAR' :0, 'GALAXY':1,'QSO':2}).astype(int) #mapping numbers #stay consistent
df2_sc

y2

y_n=df2_sc.iloc[:,6]

y_ndf=pd.DataFrame(y_n)
y_ndf

from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
encoder = LabelEncoder()
encoder.fit(y2)#taken from labeled data
encoded_Y_ts = encoder.transform(y2)
#convert integers to dummy variables (i.e. one hot encoded)
y1 = np_utils.to_categorical(encoded_Y_ts)
y1

X_train, X_test, y_train, y_test = train_test_split(X_s.values, y1, test_size=0.25, random_state=42)
y_test#Encoded #Scaled #Spliting the Data

X_train2, X_test2, y_train2, y_test2 = train_test_split(X_s , y_n, test_size=0.25, random_state=42)
y_test2#Not Encoded # Scaled

#Encoding
encoder = LabelEncoder()
encoder.fit(ytann)
encoded_yann = encoder.transform(ytann)
#convert integers to dummy variables (i.e. one hot encoded)
yann_t = np_utils.to_categorical(encoded_yann)
yann_t

Xt=df_test.drop('class',axis=1)

Xt_s=Xt/Xt.apply(max)
yt=df_test['class']#labeled

Xt_s.describe()

scalart=StandardScaler().fit(Xt)
X_scaledt=scalart.transform(Xt)
X_scaledt =pd.DataFrame(X_scaledt)
X_scaledt

#Scaled Test data with Labels
scaledt=[Xt_s,yt]
dft_sc=pd.concat(scaledt,axis=1)
dft_sc #With Labels

dft_sc['class'] = dft_sc['class'].map({'STAR' :0, 'GALAXY':1,'QSO':2}).astype(int) #mapping numbers
dft_sc#test data with numbers as labels

y_ts=dft_sc.iloc[:,6]
y_ts

"""# Linear Regression Model"""

#Linear regression
clf_linear = LinearRegression().fit(X_train2, y_train2)
#print(f"score of logistic method: {clf_linear.score(X_test, y_test)}")
yc = np.round(X_test2.dot(clf_linear.coef_.T)+clf_linear.intercept_)
yplinr = np.round(clf_linear.predict(X_test2))
assert np.array_equal(yc,yplinr) == True

acc_linear = np.sum(yc == y_test2)/y_test2.shape[0]
print(f"accuracy of linear method: {acc_linear}")

yc

yplinr

yct=np.round(Xt_s.dot(clf_linear.coef_.T)+clf_linear.intercept_)
yplinrt = np.round(clf_linear.predict(Xt_s))
assert np.array_equal(yct,yplinrt) == True

acc_linear_t = np.sum(yct == y_ts)/y_ts.shape[0]
print(f"accuracy of linear method: {acc_linear_t}")#It fails Miserably

"""IMPLEMENTING LASSO REGULARISATION"""

from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.metrics import mean_squared_error  # we will use MSE for evaluation
import matplotlib.pyplot as plt

lasso = Lasso(alpha=0.0001, fit_intercept=False, max_iter=10000)
lasso.fit(X_train2, y_train2)

lasso.coef_

ylas = np.round(X_test2.dot(lasso.coef_.T)+lasso.intercept_)
acc_linearlas = np.sum(ylas == y_test2)/y_test2.shape[0]
acc_linearlas

ylast = np.round(Xt_s.dot(lasso.coef_.T)+lasso.intercept_)
acc_linearlast = np.sum(ylast == y_ts)/y_ts.shape[0]
acc_linearlast

def plot_errors(lambdas, train_errors, test_errors, title):
    plt.figure(figsize=(16, 9))
    plt.plot(lambdas, train_errors, label="train")
    plt.plot(lambdas, test_errors, label="test")
    plt.xlabel("$\\lambda$", fontsize=14)
    plt.ylabel("MSE", fontsize=14)
    plt.title(title, fontsize=20)
    plt.legend(fontsize=14)
    plt.show()

def evaluate_model(Model, lambdas):
    training_errors = [] # we will store the error on the training set, for using each different lambda
    testing_errors = [] # and the error on the testing set
    for l in lambdas:
        # in sklearn, they refer to lambda as alpha, the name is different in different literature
        # Model will be either Lasso, Ridge or ElasticNet
        model = Model(alpha=l, max_iter=1000) # we allow max number of iterations until the model converges
        model.fit(X_train2, y_train2)

        training_predictions = model.predict(X_train2)
        training_mse = mean_squared_error(y_train2, training_predictions)
        training_errors.append(training_mse)

        testing_predictions = model.predict(X_test2)
        testing_mse = mean_squared_error(y_test2, testing_predictions)
        testing_errors.append(testing_mse)
    return training_errors, testing_errors

lambdas = np.arange(-10, 0.2, step=0.1)
lasso_train, lasso_test = evaluate_model(Lasso, lambdas)
plot_errors(lambdas, lasso_train, lasso_test, "Lasso")

"""Implementing Ridge Regularization"""

#Ridge Regularization
from sklearn import linear_model
ridge = linear_model.Ridge(alpha=0.01, fit_intercept=False)
ridge.fit(X_train2, y_train2)
ridge.coef_

yrig = np.round(X_test2.dot(ridge.coef_.T)+ridge.intercept_)
acc_linearig = np.sum(yrig == y_test2)/y_test2.shape[0]
acc_linearig

"""# Testing regression for the feature redshift"""

linX=X_s.iloc[:,0:5]
linX

liny=X_s.iloc[:,5]
liny

X_train_lin,X_test_lin,y_train_lin,y_test_lin=train_test_split(linX, liny, test_size=0.25, random_state=42)
y_test_lin

linreg = LinearRegression().fit(X_train_lin, y_train_lin)

print(linreg.score(X_test_lin, y_test_lin))

linypred=linreg.predict(X_test_lin)
mse=np.sum(y_test_lin-linypred)**2
mse**0.5 #Standard Scaling Doesn't improve the result for this case at least

plt.scatter(y_test_lin,linypred, s=0.3)
plt.xlabel('redshift')
plt.ylabel('predicted_redshift')
plt.show()

"""# Logistic Regression Model

"""

#basic model
modellr = LogisticRegression()  #Create an instance of the model.

modellr.fit(X_train2, y_train2)  # Train the model using training data

ypredlr = modellr.predict(X_test2)

print ("Accuracy = ",metrics.accuracy_score(y_test2, ypredlr))

modellro = LogisticRegression(penalty='l1',solver='saga',max_iter=10000,tol=1e-4, class_weight='balanced',random_state=0)  #Create an instance of the model.

modellro.fit(X_train2, y_train2)  # Train the model using training data

ypredlro = modellro.predict(X_test2)

print ("Accuracy = ",metrics.accuracy_score(y_test2, ypredlro))
#takes time tho #mention this

"""Cross Validation"""

from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
# create dataset

# prepare the cross-validation procedure
cvlr = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
# create model

# evaluate model
scores = cross_val_score(modellro, X_train2, y_train2, scoring='accuracy', cv=cvlr, n_jobs=-1)
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))



def acc_gen(y_test,y_pred):
    result = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:")
    print(result)
    result1 = classification_report(y_test, y_pred,zero_division=1)
    print("Classification Report:")
    print (result1)
    result2 = accuracy_score(y_test,y_pred)
    print("Accuracy:",result2)

acc_gen(y_test2,ypredlro) #optimized model on training (test part)

ypredlro_t=modellro.predict(Xt_s)
acc_gen(y_ts,ypredlro_t) #optimized model on the testing dataset

import seaborn as sns
cnfm = confusion_matrix(y_ts,ypredlro_t)

sns.heatmap(cnfm, cmap ='Blues', annot = True,fmt='d')

"""# SVM Model Implementaion"""

#basic model
svm_k=svm.SVC()#takes a bit of time
svm_k.fit(X_train2, y_train2)
ypredsvm = svm_k.predict(X_test2)
print ("Accuracy = ", metrics.accuracy_score(y_test2, ypredsvm))

acc_gen(y_test2,ypredsvm)

df20=df2_sc[::10]
X20=df20.drop('class',axis=1)
y20=df20['class']

X_train3, X_test3, y_train3, y_test3 = train_test_split(X20,y20, test_size=0.2, random_state=42)
y_test3 #Truncated Data Set for Parameter Grid Search

#Optimizing model
param_grid = {'C': [0.01,0.1, 1, 10, 100, 500,1000],  
              #'gamma': [1, 0.1, 0.08, 0.05, 0.02, 0.01, 0.001], 
              #'coef0' : [10, 1, 0.1, 0.08, 0.05, 0.02, 0.01, 0.001],
              'kernel': ['linear','poly', 'rbf', 'sigmoid']} 

#Do it on much smaller data set
grid = GridSearchCV(SVC(class_weight = 'balanced'), param_grid, verbose = False, cv = 5) 



# fitting the model for grid search 
grid.fit(X_train3, y_train3) 
# print best parameter after tuning 
print(grid.best_params_) 

# print how our model looks after hyper-parameter tuning 
print(grid.best_estimator_) 

grid_predictions = grid.predict(X_test3) 
  
# print classification report 
print(classification_report(y_test3, grid_predictions))

svm_ko=svm.SVC(C=500,class_weight='balanced',kernel='poly')#takes a bit of time #Searched Using Param Grid
svm_ko.fit(X_train2, y_train2)
ypredsvmo = svm_ko.predict(X_test2)
print ("Accuracy = ", metrics.accuracy_score(y_test2, ypredsvmo))

# prepare the cross-validation procedure
cvsvm = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
# create model

# evaluate model
scores = cross_val_score(svm_ko, X_train2, y_train2, scoring='accuracy', cv=cvsvm, n_jobs=-1)
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))

ypredsvm_t=svm_ko.predict(Xt_s)
acc_gen(y_ts,ypredsvm_t)#Not the best for classifying Quasars

cnfm_svm = confusion_matrix(y_ts,ypredsvm_t)

sns.heatmap(cnfm_svm, cmap ='Blues', annot = True, fmt='d')

"""# Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier(n_estimators=100,random_state=42,criterion='gini')
rfc.fit(X_train2,y_train2)
y_predrf=rfc.predict(X_test2)
print("Accuracy:",metrics.accuracy_score(y_test2, y_predrf))

acc_gen(y_test2,y_predrf)

cvsrf = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
# create model

# evaluate model
scores = cross_val_score(rfc, X_train2, y_train2, scoring='accuracy', cv=cvsrf, n_jobs=-1)
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))

ypredrf_t=rfc.predict(Xt_s)
ypredrf_t
acc_gen(y_ts,ypredrf_t) #problem in classifying galaxies

cnfm_rf = confusion_matrix(y_ts,ypredrf_t)

sns.heatmap(cnfm_rf, cmap ='Blues', annot = True, fmt='d')

rfcn=RandomForestClassifier(n_estimators=100,criterion='entropy',max_depth=None,
                           min_samples_split=3, min_samples_leaf=3, min_weight_fraction_leaf=0.0,
                           max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0,
                           min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None,
                           random_state=0, verbose=0, warm_start=False, class_weight='balanced',
                           ccp_alpha=0.0, max_samples=None)
rfcn.fit(X_train2,y_train2)
y_predrfn=rfcn.predict(X_test2)
print("Accuracy:",metrics.accuracy_score(y_test2, y_predrfn))
ypredrf_nt=rfcn.predict(Xt_s)
acc_gen(y_ts,ypredrf_nt)
#entropy decrease

"""# Artificial Neural Network"""

X_train4, X_test4, y_train4, y_test4 = train_test_split(X.values, y1, test_size=0.25, random_state=1)
y_test4 #Not scaled

X_train4

#Generating a Basic Model Now this model is tuned
modelann=Sequential()
modelann.add(Dense(256,activation='relu'))
modelann.add(Dropout(rate=0.2))
modelann.add(Dense(128,activation='relu'))
modelann.add(Dropout(rate=0.2))
modelann.add(Dense(64,activation='relu'))
modelann.add(Dense(64,activation='relu'))
modelann.add(Dense(3,activation='softmax'))#128 gives adam and adagrad

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val_loss',patience=10,mode='min')

def plot_training_history(history, model, loss_function, optimizer):
    figure = plt.figure()
    figure.suptitle(loss_function)
    
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['training', 'validation'], loc='best')
    plt.tight_layout()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['training', 'validation'], loc='best')
    plt.tight_layout()
    
    figure.tight_layout()
    plt.show()
    
    loss, accuracy  = model.evaluate(X_test4,y_test4, verbose=False)
    model.evaluate(Xt.values,yann_t)
    
    print(f'Test loss: {loss:.3}')
    print(f'Test accuracy: {accuracy:.3}')
    print(optimizer)

loss_functions = [
    'categorical_crossentropy',
    'categorical_hinge',
    'kullback_leibler_divergence',
    'mean_squared_error'
]
optimizers = ['sgd','adam','nadam','adagrad','rmsprop']

histories = []

for loss_function in loss_functions:
    for optimizer in optimizers:
        model = modelann
        model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])
        history = model.fit(X_train4,y_train4, batch_size=128, epochs=20, verbose=False, validation_data=(X_test4,y_test4),callbacks=[es])
        plot_training_history(history, model, loss_function, optimizer)
        histories.append(history)

from keras.callbacks import ModelCheckpoint

modelann2=Sequential()
modelann2.add(Dense(256,activation='relu',input_dim=6))
modelann.add(Dropout(rate=0.2))

modelann2.add(Dense(128,activation='relu'))

modelann2.add(Dense(64,activation='relu'))
modelann2.add(Dense(32,activation='relu'))
modelann2.add(Dense(3,activation='softmax'))
optimizers2 = ['adam','nadam','adagrad','rmsprop']

#implement Early Stopping Maybe #Keras Tuner...

from keras.models import load_model
for i in range(5):
    mc = ModelCheckpoint('modelf'+str(i+1)+'.h5', monitor='val_accuracy', mode='max', save_best_only=True,verbose=False)
    for optimizer in optimizers2: #Not Normalized
            model = modelann2
            model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
            model.fit(X_train4,y_train4,epochs=50,verbose=False,callbacks=[es,mc],validation_data=(X_test4,y_test4),batch_size=256)
            model.evaluate(X_test4,y_test4)
            model.evaluate(Xt.values,yann_t)
            
            print(optimizer)
    saved_modelf = tf.keras.models.load_model('modelf'+str(i+1)+'.h5')
    # evaluate the model

    test_acc = saved_modelf.evaluate(Xt.values,yann_t, verbose=0)
    print(test_acc)

#All models give >98%
saved_modelff = tf.keras.models.load_model('modelf5.h5')
ypredann=saved_modelff(Xt.values)

from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix

y_predannf = np.argmax(ypredann,axis=1)
yann_ta = np.argmax(yann_t,axis=1)
yann_ta

# Print f1, precision, and recall scores
print(precision_score(yann_ta, y_predannf , average="macro"))
print(recall_score(yann_ta, y_predannf , average="macro"))
print(f1_score(yann_ta, y_predannf , average="macro"))

acc_gen(yann_ta,y_predannf)

cnfm_ann = confusion_matrix(yann_ta,y_predannf)

sns.heatmap(cnfm_ann, cmap ='Blues', annot = True, fmt='d')#0-GALAXY,1-QUASAR,2-STAR Best Result So far

"""# Implementing Unsupervised Learning Method K-means Clustering"""

from sklearn.cluster import MiniBatchKMeans
total_clusters = 3
# Initialize the K-Means model
kmeans = MiniBatchKMeans(n_clusters = total_clusters,batch_size=3072)
# Fitting the model to training set
kmeans.fit(X_train2)

kmeans.labels_[3]

def retrieve_info(cluster_labels,y_train2):

 #Associates most probable label with each cluster in KMeans model
 #returns: dictionary of clusters assigned to each label

# Initializing
    reference_labels={}
# For loop to run through each label of cluster label
    for i in range(len(np.unique(kmeans.labels_))):
        index = np.where(cluster_labels == i,1,0)
        num = np.bincount(y_train2[index==1]).argmax()
        reference_labels[i] = num
    return reference_labels

r=retrieve_info(kmeans.labels_,y_train2)
print(r)

c = np.random.rand(3)
c

reference_labels = retrieve_info(kmeans.labels_,y_train2)
number_labels = np.random.rand(len(kmeans.labels_)) 
for i in range(len(kmeans.labels_)):
    number_labels[i] = reference_labels[kmeans.labels_[i]]

number_labels.astype('int')

print(number_labels[:20].astype('int'))
print(y_train2[:20])

print(accuracy_score(number_labels,y_train2))

def calculate_metrics(model,output):
    print('Number of clusters is {}\n'.format(model.n_clusters))
    print('Inertia : {}\n'.format(model.inertia_))
    print('Homogeneity : {}\n'.format(metrics.homogeneity_score(output,model.labels_)))

from sklearn import metrics
cluster_number = [10,16,36,64,144,256,512]
for i in cluster_number:
    total_clusters = len(np.unique(y_train2))
# Initialize the K-Means model
    kmeans = MiniBatchKMeans(n_clusters = i,random_state=1)
# Fitting the model to training set
    kmeans.fit(X_train2)
# Calculating the metrics
 
    calculate_metrics(kmeans,y_train2)
# Calculating reference_labels
    reference_labels = retrieve_info(kmeans.labels_,y_train2)
# ‘number_labels’ is a list which denotes the number displayed in image
    number_labels = np.zeros(len(kmeans.labels_))
    for j in range(len(kmeans.labels_)):
        number_labels[j] = reference_labels[kmeans.labels_[j]]
        number_labels[j].astype('int')
    print('Accuracy score : {}'.format(accuracy_score(number_labels,y_train2)))
    print('\n')



kmeans = MiniBatchKMeans(n_clusters = 256,random_state=1)
kmeans.fit(Xt_s)

reference_labels = retrieve_info(kmeans.labels_,y_ts)

number_labels = np.zeros(len(kmeans.labels_))
for j in range(len(kmeans.labels_)):
    number_labels[j] = reference_labels[kmeans.labels_[j]]
    number_labels[j].astype('int')
print('Accuracy score : {}'.format(accuracy_score(number_labels,y_ts)))

acc_gen(y_ts,number_labels)

cnfm_kmc = confusion_matrix(y_ts,number_labels)

sns.heatmap(cnfm_kmc, cmap ='Blues', annot = True, fmt='d')





